{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import wandb\n",
    "from torch.optim import AdamW\n",
    "from collections import defaultdict\n",
    "import psutil\n",
    "import sys\n",
    "import json\n",
    "import traceback\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from dataset import AdvancedXRayTransforms, XRayDataset, custom_collate, AdvancedBatchSampler\n",
    "from model import create_model_and_optimizer\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def seed_everything(seed=11):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * progress)))\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTrainingMonitor:\n",
    "    def __init__(self, model, class_distribution, log_dir='training_logs'):\n",
    "        \"\"\"\n",
    "        Initialize the enhanced training monitor with comprehensive tracking capabilities.\n",
    "        \n",
    "        Args:\n",
    "            model: The PyTorch model being trained\n",
    "            class_distribution: Dictionary mapping class indices to their frequencies\n",
    "            log_dir: Directory for saving training logs\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.num_classes = len(class_distribution)\n",
    "        self.class_distribution = class_distribution\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        # Initialize statistics tracking\n",
    "        self.stats = {\n",
    "            'gradient_norms': defaultdict(list),\n",
    "            'layer_metrics': defaultdict(list),\n",
    "            'class_accuracies': {i: [] for i in range(self.num_classes)},\n",
    "            'class_predictions': {i: [] for i in range(self.num_classes)},\n",
    "        }\n",
    "        \n",
    "        # Create log directory\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        # Setup gradient monitoring\n",
    "        self.setup_gradient_hooks()\n",
    "        \n",
    "        # Compute initial class weights\n",
    "        self.class_weights = self.compute_class_weights()\n",
    "\n",
    "    def setup_gradient_hooks(self):\n",
    "        \"\"\"Setup gradient monitoring hooks for all parameters.\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.register_hook(lambda grad, name=name: self._gradient_monitor(grad, name))\n",
    "\n",
    "    def _gradient_monitor(self, grad, param_name):\n",
    "        \"\"\"Monitor and stabilize gradients.\"\"\"\n",
    "        if grad is not None:\n",
    "            grad_norm = grad.norm().item()\n",
    "            self.stats['gradient_norms'][param_name].append(grad_norm)\n",
    "            \n",
    "            # Handle gradient anomalies\n",
    "            if grad_norm > 10:  # Unusually high gradients\n",
    "                grad = grad * (10 / grad_norm)  # Scale down\n",
    "            \n",
    "            if torch.isnan(grad).any() or torch.isinf(grad).any():\n",
    "                grad = torch.where(\n",
    "                    torch.isnan(grad) | torch.isinf(grad),\n",
    "                    torch.zeros_like(grad),\n",
    "                    grad\n",
    "                )\n",
    "            \n",
    "        return grad\n",
    "\n",
    "    def compute_class_weights(self):\n",
    "        \"\"\"Compute balanced class weights based on class distribution.\"\"\"\n",
    "        total_samples = sum(self.class_distribution.values())\n",
    "        weights = []\n",
    "        for i in range(self.num_classes):\n",
    "            if i in self.class_distribution and self.class_distribution[i] > 0:\n",
    "                weight = total_samples / (self.num_classes * self.class_distribution[i])\n",
    "            else:\n",
    "                weight = 1.0\n",
    "            weights.append(weight)\n",
    "        return torch.tensor(weights)\n",
    "\n",
    "    def log_batch_metrics(self, batch_idx, loss, accuracy, optimizer, outputs, targets, global_step):\n",
    "        \"\"\"Log comprehensive batch-level metrics.\"\"\"\n",
    "        # Calculate gradient norms\n",
    "        total_norm = 0\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        \n",
    "        # Get predictions and update class-wise metrics\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for class_idx in range(self.num_classes):\n",
    "            # Update class accuracies\n",
    "            mask = targets == class_idx\n",
    "            if mask.sum() > 0:\n",
    "                class_acc = (preds[mask] == targets[mask]).float().mean().item()\n",
    "                self.stats['class_accuracies'][class_idx].append(class_acc)\n",
    "            \n",
    "            # Track prediction distribution\n",
    "            class_preds = (preds == class_idx).sum().item()\n",
    "            self.stats['class_predictions'][class_idx].append(class_preds)\n",
    "        \n",
    "        # Get memory stats\n",
    "        memory_stats = psutil.Process().memory_info()\n",
    "        memory_used_gb = memory_stats.rss / (1024 ** 3)\n",
    "        \n",
    "        # Prepare metrics dictionary\n",
    "        metrics = {\n",
    "            'batch/gradient_norm': total_norm,\n",
    "            'batch/learning_rate': optimizer.param_groups[0]['lr'],\n",
    "            'batch/loss': loss,\n",
    "            'batch/accuracy': accuracy,\n",
    "            'memory/used_gb': memory_used_gb\n",
    "        }\n",
    "        \n",
    "        # Add class-specific metrics\n",
    "        for class_idx in range(self.num_classes):\n",
    "            if self.stats['class_accuracies'][class_idx]:\n",
    "                metrics[f'class_{class_idx}/accuracy'] = np.mean(\n",
    "                    self.stats['class_accuracies'][class_idx][-100:]\n",
    "                )\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log(metrics, step=global_step)\n",
    "        \n",
    "        # Check for class imbalance every 50 batches\n",
    "        if batch_idx % 50 == 0:\n",
    "            self._check_class_imbalance()\n",
    "            self._print_batch_summary(batch_idx, loss, accuracy, total_norm, memory_used_gb)\n",
    "\n",
    "    def _check_class_imbalance(self):\n",
    "        \"\"\"Check and warn about class imbalance in predictions.\"\"\"\n",
    "        pred_counts = {i: np.mean(preds[-100:]) for i, preds in self.stats['class_predictions'].items()}\n",
    "        total_preds = sum(pred_counts.values())\n",
    "        \n",
    "        if total_preds > 0:\n",
    "            pred_distribution = {k: v/total_preds for k, v in pred_counts.items()}\n",
    "            \n",
    "            for class_idx, freq in pred_distribution.items():\n",
    "                if freq < 0.1:\n",
    "                    print(f\"\\nWARNING: Class {class_idx} is severely underrepresented \"\n",
    "                          f\"({freq*100:.1f}% of predictions)\")\n",
    "                elif freq > 0.4:\n",
    "                    print(f\"\\nWARNING: Class {class_idx} is severely overrepresented \"\n",
    "                          f\"({freq*100:.1f}% of predictions)\")\n",
    "\n",
    "    def _print_batch_summary(self, batch_idx, loss, accuracy, grad_norm, memory_used_gb):\n",
    "        \"\"\"Print detailed batch summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Batch {batch_idx} Detailed Metrics:\")\n",
    "        print(\"-\"*20)\n",
    "        print(f\"Loss: {loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.2%}\")\n",
    "        print(f\"Gradient Norm: {grad_norm:.4f}\")\n",
    "        print(f\"Memory Used: {memory_used_gb:.2f}GB\")\n",
    "        print(\"\\nPer-class Accuracies:\")\n",
    "        for class_idx, accs in self.stats['class_accuracies'].items():\n",
    "            if accs:\n",
    "                recent_acc = np.mean(accs[-50:])\n",
    "                print(f\"Class {class_idx}: {recent_acc:.2%}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    def log_validation_metrics(self, val_metrics, epoch, global_step):\n",
    "        \"\"\"Log comprehensive validation metrics.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Validation Metrics - Epoch {epoch}\")\n",
    "        print(\"-\"*20)\n",
    "        print(f\"Loss: {val_metrics['val_loss']:.4f}\")\n",
    "        print(f\"Accuracy: {val_metrics['accuracy']:.2%}\")\n",
    "        print(f\"Specificity: {val_metrics['specificity']:.2%}\")\n",
    "        print(f\"Sensitivity: {val_metrics['sensitivity']:.2%}\")\n",
    "        \n",
    "        # Print confusion matrix\n",
    "        cm = val_metrics['confusion_matrix']\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        # Calculate and print per-class metrics\n",
    "        print(\"\\nPer-class Metrics:\")\n",
    "        for i in range(self.num_classes):\n",
    "            tp = cm[i, i]\n",
    "            fp = cm[:, i].sum() - tp\n",
    "            fn = cm[i, :].sum() - tp\n",
    "            tn = cm.sum() - (tp + fp + fn)\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            print(f\"\\nClass {i}:\")\n",
    "            print(f\"Precision: {precision:.2%}\")\n",
    "            print(f\"Recall: {recall:.2%}\")\n",
    "            print(f\"F1-Score: {f1:.2%}\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log({\n",
    "            'val/loss': val_metrics['val_loss'],\n",
    "            'val/accuracy': val_metrics['accuracy'],\n",
    "            'val/specificity': val_metrics['specificity'],\n",
    "            'val/sensitivity': val_metrics['sensitivity']\n",
    "        }, step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_data_loading(train_loader):\n",
    "    \"\"\"Verify data loading and print initial statistics\"\"\"\n",
    "    print(\"\\nVerifying data loading...\")\n",
    "    try:\n",
    "        batch = next(iter(train_loader))\n",
    "        inputs, targets_a, targets_b, lams = batch\n",
    "        \n",
    "        print(\"\\nInitial batch statistics:\")\n",
    "        print(f\"Input shape: {inputs.shape}\")\n",
    "        print(f\"Memory usage per batch: {inputs.element_size() * inputs.nelement() / (1024**2):.2f}MB\")\n",
    "        print(f\"Target A distribution: {torch.bincount(targets_a)}\")\n",
    "        print(f\"Target B distribution: {torch.bincount(targets_b)}\")\n",
    "        print(f\"Lambda range: {lams.min().item():.3f} - {lams.max().item():.3f}\")\n",
    "        print(\"\\nData loading verification completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during data loading verification: {str(e)}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebugMonitor:\n",
    "    def __init__(self):\n",
    "        self.grad_norms = defaultdict(list)\n",
    "        self.activation_stats = defaultdict(list)\n",
    "        self.weight_stats = defaultdict(list)\n",
    "        \n",
    "    def update_grad_stats(self, model):\n",
    "        \"\"\"Track gradient statistics for each parameter\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                self.grad_norms[name].append(param.grad.norm().item())\n",
    "                \n",
    "    def update_activation_stats(self, name, tensor):\n",
    "        \"\"\"Track activation statistics\"\"\"\n",
    "        stats = {\n",
    "            'mean': tensor.mean().item(),\n",
    "            'std': tensor.std().item(),\n",
    "            'max': tensor.max().item(),\n",
    "            'min': tensor.min().item()\n",
    "        }\n",
    "        self.activation_stats[name].append(stats)\n",
    "        \n",
    "    def update_weight_stats(self, model):\n",
    "        \"\"\"Track weight statistics\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            stats = {\n",
    "                'mean': param.data.mean().item(),\n",
    "                'std': param.data.std().item(),\n",
    "                'max': param.data.max().item(),\n",
    "                'min': param.data.min().item()\n",
    "            }\n",
    "            self.weight_stats[name].append(stats)\n",
    "            \n",
    "    def log_to_wandb(self, step):\n",
    "        \"\"\"Log statistics to W&B\"\"\"\n",
    "        # Log gradient norms\n",
    "        for name, norms in self.grad_norms.items():\n",
    "            if norms:  # Only log if we have data\n",
    "                wandb.log({f'grad_norm/{name}': norms[-1]}, step=step)\n",
    "        \n",
    "        # Log activation statistics\n",
    "        for name, stats_list in self.activation_stats.items():\n",
    "            if stats_list:\n",
    "                latest_stats = stats_list[-1]\n",
    "                for stat_name, value in latest_stats.items():\n",
    "                    wandb.log({f'activation/{name}/{stat_name}': value}, step=step)\n",
    "        \n",
    "        # Log weight statistics\n",
    "        for name, stats_list in self.weight_stats.items():\n",
    "            if stats_list:\n",
    "                latest_stats = stats_list[-1]\n",
    "                for stat_name, value in latest_stats.items():\n",
    "                    wandb.log({f'weight/{name}/{stat_name}': value}, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"Clear unused memory caches.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        # MPS (Apple Silicon) doesn't need explicit cleanup\n",
    "        pass\n",
    "    \n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(config):\n",
    "    \"\"\"Create optimized data loaders with balanced sampling and verification\"\"\"\n",
    "    \n",
    "    # Data augmentation and preprocessing\n",
    "    train_transform = AdvancedXRayTransforms.get_train_transform()\n",
    "    val_transform = AdvancedXRayTransforms.get_val_transform()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = XRayDataset(\n",
    "        root_dir=config['train_dir'],\n",
    "        transform=train_transform,\n",
    "        phase='train',\n",
    "        mixup_alpha=0.2\n",
    "    )\n",
    "    \n",
    "    val_dataset = XRayDataset(\n",
    "        root_dir=config['val_dir'],\n",
    "        transform=val_transform,\n",
    "        phase='val'\n",
    "    )\n",
    "    \n",
    "    train_sampler = AdvancedBatchSampler(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=32,\n",
    "        balance_strategy='oversample',  # or 'weights' or 'stratified'\n",
    "        oversample_multiplier=1.2,\n",
    "        dynamic_balance=True,\n",
    "        min_class_samples=2\n",
    "    )\n",
    "\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_sampler=train_sampler,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate\n",
    "    )\n",
    "\n",
    "    # Verify batch distribution\n",
    "    print(\"\\nVerifying training batch distribution...\")\n",
    "    verify_batch_distribution(train_loader, num_batches=5)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def verify_batch_distribution(loader, num_batches=5):\n",
    "    \"\"\"Verify the class distribution in the first few batches\"\"\"\n",
    "    class_counts = {i: 0 for i in range(5)}\n",
    "    for i, (_, targets_a, _, _) in enumerate(loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        for target in targets_a:\n",
    "            class_counts[target.item()] += 1\n",
    "    \n",
    "    total = sum(class_counts.values())\n",
    "    print(\"\\nBatch distribution check:\")\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"Class {cls}: {count} samples ({count/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, scheduler, device, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.global_step = 0\n",
    "        self.best_val_acc = 0\n",
    "        \n",
    "        # Initialize monitoring system\n",
    "        self.training_monitor = EnhancedTrainingMonitor(\n",
    "            model=model,\n",
    "            class_distribution=config['class_distribution'],\n",
    "            log_dir='training_logs'\n",
    "        )\n",
    "        \n",
    "        # Enable gradient monitoring in wandb\n",
    "        wandb.watch(model, log=\"gradients\", log_freq=200)\n",
    "        \n",
    "    def train_one_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "\n",
    "        if epoch == 0:\n",
    "            print(f\"\\nVerifying batch distribution for epoch {epoch}...\")\n",
    "            verify_batch_distribution(self.train_loader, num_batches=5)\n",
    "        \n",
    "        # Calculate mixup alpha based on epoch\n",
    "        current_mixup_alpha = self.config['mixup_alpha'] * min(1.0, epoch / self.config['warmup_mixup_epochs'])\n",
    "        \n",
    "        pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader))\n",
    "        \n",
    "        for batch_idx, (inputs, targets_a, targets_b, lams) in pbar:\n",
    "            try:\n",
    "                # Move to device\n",
    "                inputs = inputs.to(self.device, non_blocking=True)\n",
    "                targets_a = targets_a.to(self.device, non_blocking=True)\n",
    "                targets_b = targets_b.to(self.device, non_blocking=True)\n",
    "                lams = lams.to(self.device, non_blocking=True)\n",
    "                \n",
    "                # Gradient accumulation steps\n",
    "                is_accumulation_step = (batch_idx + 1) % self.config['accumulation_steps'] != 0\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                if epoch >= self.config['warmup_epochs']:\n",
    "                    loss_a = self.criterion(outputs, targets_a)\n",
    "                    loss_b = self.criterion(outputs, targets_b)\n",
    "                    loss = (lams * loss_a + (1 - lams) * loss_b).mean()\n",
    "                else:\n",
    "                    loss = self.criterion(outputs, targets_a)\n",
    "                \n",
    "                # Scale loss for gradient accumulation\n",
    "                loss = loss / self.config['accumulation_steps']\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                if not is_accumulation_step:\n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(),\n",
    "                        self.config['gradient_clip_val']\n",
    "                    )\n",
    "                    \n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if self.scheduler is not None:\n",
    "                        self.scheduler.step()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                with torch.no_grad():\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    targets = targets_a if epoch < self.config['warmup_epochs'] else torch.where(lams.view(-1) > 0.5, targets_a, targets_b)\n",
    "                    accuracy = (preds == targets).float().mean()\n",
    "                \n",
    "                # Update metrics\n",
    "                running_loss += loss.item() * self.config['accumulation_steps']\n",
    "                running_acc += accuracy.item()\n",
    "\n",
    "                # Log batch metrics\n",
    "                if batch_idx % 50 == 0:\n",
    "                    metrics = {\n",
    "                        'batch/loss': loss.item() * self.config['accumulation_steps'],\n",
    "                        'batch/accuracy': accuracy.item(),\n",
    "                        'batch/learning_rate': self.optimizer.param_groups[0]['lr'],\n",
    "                        'batch/memory_used': psutil.Process().memory_info().rss / 1024 / 1024 / 1024  # GB\n",
    "                    }\n",
    "                    \n",
    "                    wandb.log(metrics, step=self.global_step)\n",
    "                    \n",
    "                    self.training_monitor.log_batch_metrics(\n",
    "                        batch_idx=batch_idx,\n",
    "                        loss=loss.item() * self.config['accumulation_steps'],\n",
    "                        accuracy=accuracy.item(),\n",
    "                        optimizer=self.optimizer,\n",
    "                        outputs=outputs,\n",
    "                        targets=targets,\n",
    "                        global_step=self.global_step\n",
    "                    )\n",
    "                    \n",
    "                    self.global_step += 1\n",
    "                \n",
    "                # Clear memory periodically\n",
    "                if batch_idx % self.config['clear_cache_freq'] == 0:\n",
    "                    clear_memory()\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_description(\n",
    "                    f'Epoch {epoch} | Loss: {running_loss/(batch_idx+1):.4f} | '\n",
    "                    f'Acc: {running_acc/(batch_idx+1):.4f}'\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                clear_memory()\n",
    "                continue\n",
    "        \n",
    "        return running_loss / len(self.train_loader), running_acc / len(self.train_loader)\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets_a, _, _ in tqdm(self.val_loader, desc='Validating'):\n",
    "                inputs = inputs.to(self.device, non_blocking=True)\n",
    "                targets_a = targets_a.to(self.device, non_blocking=True)\n",
    "                \n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets_a)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                targets.extend(targets_a.cpu().numpy())\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        targets = np.array(targets)\n",
    "        \n",
    "        metrics = self._compute_validation_metrics(val_loss, predictions, targets)\n",
    "        \n",
    "        # Log validation metrics with current global step\n",
    "        self.training_monitor.log_validation_metrics(metrics, self.current_epoch, self.global_step)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def _compute_validation_metrics(self, val_loss, predictions, targets):\n",
    "        \"\"\"Compute comprehensive validation metrics\"\"\"\n",
    "        accuracy = np.mean(predictions == targets)\n",
    "        specificity = self._calculate_specificity(targets, predictions)\n",
    "        sensitivity = self._calculate_sensitivity(targets, predictions)\n",
    "        cm = confusion_matrix(targets, predictions)\n",
    "\n",
    "        metrics = {\n",
    "            'val_loss': val_loss / len(self.val_loader),\n",
    "            'accuracy': accuracy,\n",
    "            'specificity': specificity,\n",
    "            'sensitivity': sensitivity,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    def _plot_confusion_matrix(self, cm, epoch):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix - Epoch {epoch}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        wandb.log({\"confusion_matrix\": wandb.Image(plt)}, step=self.global_step)\n",
    "        plt.close()\n",
    "        \n",
    "    def _calculate_specificity(self, y_true, y_pred):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        fp = cm.sum(axis=0) - np.diag(cm)\n",
    "        tn = cm.sum() - (fp + cm.sum(axis=1) - np.diag(cm) + np.diag(cm))\n",
    "        specificity = tn / (tn + fp)\n",
    "        return np.mean(specificity)\n",
    "    \n",
    "    def _calculate_sensitivity(self, y_true, y_pred):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tp = np.diag(cm)\n",
    "        fn = cm.sum(axis=1) - tp\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        return np.mean(sensitivity)\n",
    "    \n",
    "    def train(self):\n",
    "        # Add verification before training starts\n",
    "        verify_data_loading(self.train_loader)\n",
    "        \n",
    "        patience_counter = 0\n",
    "        for epoch in range(self.config['num_epochs']):\n",
    "            self.current_epoch = epoch\n",
    "            print(f'\\nEpoch {epoch+1}/{self.config[\"num_epochs\"]}')\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_acc = self.train_one_epoch(epoch)\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = self.validate()\n",
    "            \n",
    "            # Log epoch metrics\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'train/loss': train_loss,\n",
    "                'train/accuracy': train_acc,\n",
    "                'val/loss': val_metrics['val_loss'],\n",
    "                'val/accuracy': val_metrics['accuracy'],\n",
    "                'val/specificity': val_metrics['specificity'],\n",
    "                'val/sensitivity': val_metrics['sensitivity'],\n",
    "                'learning_rate': self.optimizer.param_groups[0]['lr']\n",
    "            }, step=self.global_step)\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            self._plot_confusion_matrix(val_metrics['confusion_matrix'], epoch)\n",
    "            \n",
    "            # Update learning rate scheduler\n",
    "            if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                self.scheduler.step(val_metrics['accuracy'])\n",
    "            elif self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_metrics['accuracy'] > self.best_val_acc:\n",
    "                self.best_val_acc = val_metrics['accuracy']\n",
    "                patience_counter = 0\n",
    "                self._save_checkpoint(epoch, val_metrics)\n",
    "                print(f'\\nBest val accuracy updated to: {val_metrics[\"accuracy\"]}')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= self.config['early_stopping_patience']:\n",
    "                print(f'\\nEarly stopping triggered after {epoch + 1} epochs')\n",
    "                break\n",
    "            \n",
    "            self.global_step += 1\n",
    "        \n",
    "        return self.best_val_acc\n",
    "    \n",
    "    def _save_checkpoint(self, epoch, val_metrics):\n",
    "        \"\"\"Save model checkpoint with monitoring stats\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n",
    "            'best_val_acc': val_metrics['accuracy'],\n",
    "            'config': self.config,\n",
    "            'monitoring_stats': self.training_monitor.stats,\n",
    "            'global_step': self.global_step\n",
    "        }\n",
    "        torch.save(\n",
    "            checkpoint,\n",
    "            f'{self.config[\"output_dir\"]}/best_model.pth'\n",
    "        )\n",
    "        wandb.log({\"best_val_accuracy\": val_metrics['accuracy']}, step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xray_model(config):\n",
    "    \"\"\"\n",
    "    Enhanced training function with improved initialization, monitoring, and error handling\n",
    "    \"\"\"\n",
    "    # Set device with better handling\n",
    "    device = _setup_device()\n",
    "    \n",
    "    # Set deterministic behavior for reproducibility\n",
    "    _set_deterministic(seed=config.get('seed', 11))\n",
    "    \n",
    "    # Validate config\n",
    "    _validate_config(config)\n",
    "    \n",
    "    # Create output directory with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_dir = os.path.join(config['output_dir'], timestamp)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    config['output_dir'] = output_dir  # Update config with new path\n",
    "    \n",
    "    # Save config for reproducibility\n",
    "    with open(os.path.join(output_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    try:\n",
    "        # Initialize data loaders with error handling\n",
    "        train_loader, val_loader = create_data_loaders(config)\n",
    "        \n",
    "        # Create model, optimizer, and criterion\n",
    "        model, optimizer, criterion = create_model_and_optimizer(config)\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "        \n",
    "        # Enhanced scheduler initialization\n",
    "        scheduler = _create_scheduler(optimizer, train_loader, config)\n",
    "        \n",
    "        # Initialize trainer with memory profiling\n",
    "        trainer = IntegratedTrainer(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        # Optional data verification\n",
    "        if config.get('verify_data', True):\n",
    "            print(\"\\nVerifying data loading...\")\n",
    "            verify_data_loading(train_loader)\n",
    "            \n",
    "        # Log hardware info and memory usage\n",
    "        _log_system_info()\n",
    "        \n",
    "        # Train model with error handling\n",
    "        best_acc = trainer.train()\n",
    "        \n",
    "        # Save training summary\n",
    "        _save_training_summary(config, best_acc, output_dir)\n",
    "        \n",
    "        print(f\"\\nTraining completed successfully.\")\n",
    "        print(f\"Best validation accuracy: {best_acc:.4f}\")\n",
    "        print(f\"Model and logs saved to: {output_dir}\")\n",
    "        \n",
    "        return best_acc, output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def _setup_device():\n",
    "    \"\"\"Set up and return the appropriate device with better error handling\"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS device\")\n",
    "        # Verify MPS is working properly\n",
    "        try:\n",
    "            torch.zeros(1).to(device)\n",
    "        except Exception as e:\n",
    "            print(f\"MPS initialization failed: {e}\")\n",
    "            print(\"Falling back to CPU\")\n",
    "            device = torch.device(\"cpu\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU device\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "def _set_deterministic(seed):\n",
    "    \"\"\"Set seeds and deterministic behavior\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "    \n",
    "    # Enable deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def _validate_config(config):\n",
    "    \"\"\"Validate essential config parameters\"\"\"\n",
    "    required_keys = [\n",
    "        'num_epochs', 'output_dir', 'class_distribution',\n",
    "        'warmup_epochs', 'scheduler_cycles'\n",
    "    ]\n",
    "    \n",
    "    for key in required_keys:\n",
    "        if key not in config:\n",
    "            raise ValueError(f\"Missing required config parameter: {key}\")\n",
    "            \n",
    "    if config['warmup_epochs'] >= config['num_epochs']:\n",
    "        raise ValueError(\"warmup_epochs should be less than num_epochs\")\n",
    "\n",
    "def _create_scheduler(optimizer, train_loader, config):\n",
    "    \"\"\"Create learning rate scheduler with proper initialization\"\"\"\n",
    "    num_training_steps = len(train_loader) * config['num_epochs']\n",
    "    num_warmup_steps = len(train_loader) * config['warmup_epochs']\n",
    "    \n",
    "    return get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "        num_cycles=config['scheduler_cycles']\n",
    "    )\n",
    "\n",
    "def _log_system_info():\n",
    "    \"\"\"Log system information and memory usage\"\"\"\n",
    "    print(\"\\nSystem Information:\")\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"Number of CPUs: {os.cpu_count()}\")\n",
    "    \n",
    "    # Memory information\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "    print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "    \n",
    "    # GPU information if available\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"MPS device available\")\n",
    "\n",
    "def _save_training_summary(config, best_acc, output_dir):\n",
    "    \"\"\"Save training summary to file\"\"\"\n",
    "    summary = {\n",
    "        'best_accuracy': float(best_acc),\n",
    "        'training_completed': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(output_dir, 'training_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvignesh-rox03\u001b[0m (\u001b[33mvignesh-rox03-vellore-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/Viku/GitHub/Cloned/knee_osteoarthritis_detection/Code/Vignesh/Classification_V2/wandb/run-20250115_213605-j8a2apt1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vignesh-rox03-vellore-institute-of-technology/xray-classification-v2/runs/j8a2apt1' target=\"_blank\">improved-training-run-v5</a></strong> to <a href='https://wandb.ai/vignesh-rox03-vellore-institute-of-technology/xray-classification-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vignesh-rox03-vellore-institute-of-technology/xray-classification-v2' target=\"_blank\">https://wandb.ai/vignesh-rox03-vellore-institute-of-technology/xray-classification-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vignesh-rox03-vellore-institute-of-technology/xray-classification-v2/runs/j8a2apt1' target=\"_blank\">https://wandb.ai/vignesh-rox03-vellore-institute-of-technology/xray-classification-v2/runs/j8a2apt1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n",
      "\n",
      "Error during training: 1 validation error for InitSchema\n",
      "size\n",
      "  Input should be a valid tuple [type=tuple_type, input_value=224, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/tuple_type\n",
      "\n",
      "Error during training: 1 validation error for InitSchema\n",
      "size\n",
      "  Input should be a valid tuple [type=tuple_type, input_value=224, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/tuple_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_2/xx5z8xdj6j98wh4vt2b59jz80000gp/T/ipykernel_59648/2248032029.py\", line 26, in train_xray_model\n",
      "    train_loader, val_loader = create_data_loaders(config)\n",
      "  File \"/var/folders/_2/xx5z8xdj6j98wh4vt2b59jz80000gp/T/ipykernel_59648/4202239987.py\", line 5, in create_data_loaders\n",
      "    train_transform = AdvancedXRayTransforms.get_train_transform()\n",
      "  File \"/Users/Viku/GitHub/Cloned/knee_osteoarthritis_detection/Code/Vignesh/Classification_V2/dataset.py\", line 22, in get_train_transform\n",
      "    A.RandomResizedCrop(\n",
      "  File \"/Users/Viku/GitHub/Cloned/knee_osteoarthritis_detection/.venv/lib/python3.10/site-packages/albumentations/core/validation.py\", line 35, in custom_init\n",
      "    config = dct[\"InitSchema\"](**full_kwargs)\n",
      "  File \"/Users/Viku/GitHub/Cloned/knee_osteoarthritis_detection/.venv/lib/python3.10/site-packages/pydantic/main.py\", line 214, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for InitSchema\n",
      "size\n",
      "  Input should be a valid tuple [type=tuple_type, input_value=224, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/tuple_type\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">improved-training-run-v5</strong> at: <a href='https://wandb.ai/vignesh-rox03-vellore-institute-of-technology/xray-classification-v2/runs/j8a2apt1' target=\"_blank\">https://wandb.ai/vignesh-rox03-vellore-institute-of-technology/xray-classification-v2/runs/j8a2apt1</a><br> View project at: <a href='https://wandb.ai/vignesh-rox03-vellore-institute-of-technology/xray-classification-v2' target=\"_blank\">https://wandb.ai/vignesh-rox03-vellore-institute-of-technology/xray-classification-v2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250115_213605-j8a2apt1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for InitSchema\nsize\n  Input should be a valid tuple [type=tuple_type, input_value=224, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.10/v/tuple_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mError during training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[12], line 69\u001b[0m\n\u001b[1;32m     62\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m     63\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxray-classification-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     64\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     65\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimproved-training-run-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mtrain_xray_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining interrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m, in \u001b[0;36mtrain_xray_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     22\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(config, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Initialize data loaders with error handling\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     train_loader, val_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_data_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Create model, optimizer, and criterion\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     model, optimizer, criterion \u001b[38;5;241m=\u001b[39m create_model_and_optimizer(config)\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mcreate_data_loaders\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create optimized data loaders with balanced sampling and verification\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Data augmentation and preprocessing\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_transform \u001b[38;5;241m=\u001b[39m \u001b[43mAdvancedXRayTransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m val_transform \u001b[38;5;241m=\u001b[39m AdvancedXRayTransforms\u001b[38;5;241m.\u001b[39mget_val_transform()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/Cloned/knee_osteoarthritis_detection/Code/Vignesh/Classification_V2/dataset.py:22\u001b[0m, in \u001b[0;36mAdvancedXRayTransforms.get_train_transform\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_train_transform\u001b[39m():\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# Careful geometric transforms for knee alignment\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m         \u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomResizedCrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     28\u001b[0m         A\u001b[38;5;241m.\u001b[39mShiftScaleRotate(\n\u001b[1;32m     29\u001b[0m             shift_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m,\n\u001b[1;32m     30\u001b[0m             scale_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[1;32m     31\u001b[0m             rotate_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     32\u001b[0m             border_mode\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mBORDER_CONSTANT,\n\u001b[1;32m     33\u001b[0m             value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     34\u001b[0m             p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     35\u001b[0m         ),\n\u001b[1;32m     36\u001b[0m         \n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# X-ray specific enhancements\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         A\u001b[38;5;241m.\u001b[39mOneOf([\n\u001b[1;32m     39\u001b[0m             A\u001b[38;5;241m.\u001b[39mGaussNoise(var_limit\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m20\u001b[39m), p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m),\n\u001b[1;32m     40\u001b[0m             A\u001b[38;5;241m.\u001b[39mGaussianBlur(blur_limit\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m),\n\u001b[1;32m     41\u001b[0m         ], p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m),\n\u001b[1;32m     42\u001b[0m         \n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m# Enhance bone details\u001b[39;00m\n\u001b[1;32m     44\u001b[0m         A\u001b[38;5;241m.\u001b[39mCLAHE(\n\u001b[1;32m     45\u001b[0m             clip_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m,\n\u001b[1;32m     46\u001b[0m             tile_grid_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m     47\u001b[0m             p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     48\u001b[0m         ),\n\u001b[1;32m     49\u001b[0m         \n\u001b[1;32m     50\u001b[0m         \u001b[38;5;66;03m# Contrast adjustment for better bone visibility\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         A\u001b[38;5;241m.\u001b[39mRandomBrightnessContrast(\n\u001b[1;32m     52\u001b[0m             brightness_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m,\n\u001b[1;32m     53\u001b[0m             contrast_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m,\n\u001b[1;32m     54\u001b[0m             p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     55\u001b[0m         ),\n\u001b[1;32m     56\u001b[0m         \n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# X-ray specific normalization\u001b[39;00m\n\u001b[1;32m     58\u001b[0m         A\u001b[38;5;241m.\u001b[39mNormalize(\n\u001b[1;32m     59\u001b[0m             mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.0\u001b[39m],  \u001b[38;5;66;03m# Changed to 0 for grayscale\u001b[39;00m\n\u001b[1;32m     60\u001b[0m             std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1.0\u001b[39m],   \u001b[38;5;66;03m# Keep unit variance\u001b[39;00m\n\u001b[1;32m     61\u001b[0m             max_pixel_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     62\u001b[0m         ),\n\u001b[1;32m     63\u001b[0m         ToTensorV2()\n\u001b[1;32m     64\u001b[0m     ])\n",
      "File \u001b[0;32m~/GitHub/Cloned/knee_osteoarthritis_detection/.venv/lib/python3.10/site-packages/albumentations/core/validation.py:35\u001b[0m, in \u001b[0;36mValidatedTransformMeta.__new__.<locals>.custom_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m         full_kwargs[parameter_name] \u001b[38;5;241m=\u001b[39m parameter\u001b[38;5;241m.\u001b[39mdefault\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# No try-except block needed as we want the exception to propagate naturally\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mdct\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInitSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m validated_kwargs \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmodel_dump()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name_arg \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/GitHub/Cloned/knee_osteoarthritis_detection/.venv/lib/python3.10/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for InitSchema\nsize\n  Input should be a valid tuple [type=tuple_type, input_value=224, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.10/v/tuple_type"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    # Model parameters\n",
    "    'num_classes': 5,\n",
    "    'batch_size': 32,  # Reduced for M1 memory constraints\n",
    "    'num_epochs': 100,  # Reduced but should be sufficient with better optimization\n",
    "    \n",
    "    # Optimization parameters\n",
    "    'base_learning_rate': 1e-4,  # Reduced for more stable training\n",
    "    'weight_decay': 2e-5,  # Reduced to prevent over-regularization\n",
    "    'gradient_clip_val': 0.5,  # More aggressive clipping for stability\n",
    "    'accumulation_steps': 4,  # Added to effectively increase batch size\n",
    "    \n",
    "    # Loss function parameters\n",
    "    'focal_loss_params': {\n",
    "        'gamma': 0.5,  # Reduced from 2.0 for more stable training\n",
    "        'smoothing': 0.01  # Reduced smoothing for medical images\n",
    "    },\n",
    "\n",
    "    # Memory optimization for M1\n",
    "    'memory_management': {\n",
    "        'gradient_checkpointing': True,  # Enable to save memory\n",
    "        'empty_cache_freq': 3\n",
    "    },\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    'scheduler_cycles': 1,  # Full cosine cycle\n",
    "    'warmup_epochs': 3,  # Increased warmup period\n",
    "    'warmup_mixup_epochs': 5,\n",
    "    \n",
    "    # Mixup parameters\n",
    "    'mixup_alpha': 0.1,  # Reduced for medical domain\n",
    "    \n",
    "    # Early stopping\n",
    "    'early_stopping_patience': 15,  # Increased to allow convergence\n",
    "    \n",
    "    # Data parameters - keeping your existing paths\n",
    "    'train_dir': '/Users/Viku/Datasets/Medical/Knee/train',\n",
    "    'val_dir': '/Users/Viku/Datasets/Medical/Knee/val',\n",
    "    'output_dir': 'training_output_v2',\n",
    "    'class_distribution': {0: 2286, 1: 1046, 2: 1516, 3: 757, 4: 173},\n",
    "    \n",
    "    # System parameters for M1\n",
    "    'num_workers': 2,  # Reduced for M1\n",
    "    'pin_memory': False,\n",
    "    'prefetch_factor': 2,\n",
    "    'clear_cache_freq': 2,\n",
    "    \n",
    "    # Monitoring\n",
    "    'verify_data': True,\n",
    "    'monitor_gradients': True,\n",
    "    'save_checkpoint_freq': 3,\n",
    "    'gradient_logging_freq': 200,\n",
    "    \n",
    "    # M1 specific\n",
    "    'mps_memory_limit': 12 * 1024 * 1024 * 1024,  # 12GB limit\n",
    "    'compile_model': False  # Disabled for better stability\n",
    "}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"xray-classification-v2\",\n",
    "        config=config,\n",
    "        name=\"improved-training-run-v5\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        train_xray_model(config)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {str(e)}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Create backup with timestamp\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# current_model_path = \"training_output/best_model.pth\"\n",
    "# backup_path = f\"training_output/best_model_acc60_{timestamp}.pth\"\n",
    "\n",
    "# if os.path.exists(current_model_path):\n",
    "#     shutil.copy(current_model_path, backup_path)\n",
    "#     print(f\"Backed up current model to: {backup_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer, criterion = create_model_and_optimizer(config)\n",
    "# model = model.to(device)\n",
    "\n",
    "# pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# print( pytorch_total_params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_with_kfold(config, device, num_workers=2):\n",
    "#     print(\"Initializing k-fold training...\")\n",
    "#     output_dir = 'output'\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     n_splits = 5\n",
    "#     fold_results = []\n",
    "#     fold_histories = []\n",
    "    \n",
    "#     for fold in range(n_splits):\n",
    "#         print(f\"\\nStarting Fold {fold + 1}/{n_splits}\")\n",
    "#         fold_dir = f'{output_dir}/fold_{fold+1}'\n",
    "#         os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "#         # Calculate class weights\n",
    "#         class_counts = torch.tensor([config['class_distribution'][i] for i in range(config['num_classes'])])\n",
    "#         class_weights = 1.0 / class_counts\n",
    "#         class_weights = class_weights / class_weights.sum() * config['num_classes']\n",
    "#         class_weights = class_weights.to(device)\n",
    "        \n",
    "#         # Create datasets with full transforms\n",
    "#         train_transform, val_transform = get_improved_transforms()\n",
    "#         train_dataset = XRayDataset(config['train_dir'], transform=train_transform, mixup_alpha=0.2)\n",
    "#         val_dataset = XRayDataset(config['val_dir'], transform=val_transform)\n",
    "        \n",
    "#         # Use optimized sampler for training\n",
    "#         train_sampler = OptimizedBatchSampler(train_dataset, config['batch_size'])\n",
    "        \n",
    "#         # Create data loaders\n",
    "#         train_loader = DataLoader(\n",
    "#             train_dataset,\n",
    "#             batch_sampler=train_sampler,\n",
    "#             num_workers=num_workers,\n",
    "#             pin_memory=True\n",
    "#         )\n",
    "        \n",
    "#         val_loader = DataLoader(\n",
    "#             val_dataset,\n",
    "#             batch_size=config['batch_size'],\n",
    "#             shuffle=False,\n",
    "#             num_workers=num_workers,\n",
    "#             pin_memory=True\n",
    "#         )\n",
    "        \n",
    "#         # Initialize model\n",
    "#         model = IntegratedXRayClassifier(\n",
    "#             num_classes=config['num_classes'],\n",
    "#             hidden_size=config['hidden_size']\n",
    "#         ).to(device)\n",
    "        \n",
    "#         # Initialize the balanced optimizer and loss\n",
    "#         optimizer = BalancedOptimizer(\n",
    "#             model, \n",
    "#             config['base_learning_rate'],\n",
    "#             config['weight_decay'],\n",
    "#             config['class_distribution']\n",
    "#         )\n",
    "\n",
    "#         criterion = ImprovedCombinedLoss(\n",
    "#             class_weights=optimizer.class_weights.to(device)\n",
    "# )\n",
    "        \n",
    "#         # Scheduler setup\n",
    "#         num_warmup_steps = 100\n",
    "#         num_training_steps = len(train_loader) * config['num_epochs']\n",
    "#         scheduler = get_linear_schedule_with_warmup(\n",
    "#             optimizer,\n",
    "#             num_warmup_steps=num_warmup_steps,\n",
    "#             num_training_steps=num_training_steps\n",
    "#         )\n",
    "        \n",
    "#         # Training loop variables\n",
    "#         best_val_acc = 0\n",
    "#         patience_counter = 0\n",
    "#         best_val_preds = None\n",
    "#         best_val_targets = None\n",
    "#         best_val_probs = None\n",
    "        \n",
    "#         fold_history = {\n",
    "#             'train_loss': [], 'train_acc': [],\n",
    "#             'val_loss': [], 'val_acc': []\n",
    "#         }\n",
    "        \n",
    "#         # Training loop\n",
    "#         for epoch in range(config['num_epochs']):\n",
    "#             print(f'\\nEpoch {epoch+1}/{config[\"num_epochs\"]}')\n",
    "            \n",
    "#             train_loss, train_acc = train_one_epoch(\n",
    "#                 model, train_loader, criterion, optimizer, scheduler, device,\n",
    "#                 config['gradient_accumulation_steps']\n",
    "#             )\n",
    "            \n",
    "#             val_loss, val_acc, val_preds, val_targets, val_probs = validate(\n",
    "#                 model, val_loader, criterion, device\n",
    "#             )\n",
    "            \n",
    "#             # Update history\n",
    "#             fold_history['train_loss'].append(train_loss)\n",
    "#             fold_history['train_acc'].append(train_acc)\n",
    "#             fold_history['val_loss'].append(val_loss)\n",
    "#             fold_history['val_acc'].append(val_acc)\n",
    "            \n",
    "#             print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "#             print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "            \n",
    "#             # Save plots\n",
    "#             save_plots(epoch, fold, \n",
    "#                       {'loss': fold_history['train_loss'], 'acc': fold_history['train_acc']},\n",
    "#                       {'loss': fold_history['val_loss'], 'acc': fold_history['val_acc']},\n",
    "#                       fold_dir)\n",
    "            \n",
    "#             # Check for improvement\n",
    "#             if val_acc > best_val_acc:\n",
    "#                 best_val_acc = val_acc\n",
    "#                 best_val_preds = val_preds\n",
    "#                 best_val_targets = val_targets\n",
    "#                 best_val_probs = val_probs\n",
    "                \n",
    "#                 # Save best model\n",
    "#                 torch.save({\n",
    "#                     'fold': fold,\n",
    "#                     'epoch': epoch,\n",
    "#                     'model_state_dict': model.state_dict(),\n",
    "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                     'scheduler_state_dict': scheduler.state_dict(),\n",
    "#                     'best_val_acc': best_val_acc,\n",
    "#                     'config': config\n",
    "#                 }, f'{fold_dir}/best_model.pth')\n",
    "                \n",
    "#                 patience_counter = 0\n",
    "#             else:\n",
    "#                 patience_counter += 1\n",
    "            \n",
    "#             if patience_counter >= config['early_stopping_patience']:\n",
    "#                 print('Early stopping triggered')\n",
    "#                 break\n",
    "        \n",
    "#         fold_histories.append(fold_history)\n",
    "#         fold_results.append(best_val_acc)\n",
    "        \n",
    "#         # Save final metrics\n",
    "#         with open(f'{fold_dir}/classification_report.txt', 'w') as f:\n",
    "#             f.write(classification_report(best_val_targets, best_val_preds))\n",
    "    \n",
    "#     # Print final results\n",
    "#     print(\"\\nCross-validation results:\")\n",
    "#     for fold, acc in enumerate(fold_results):\n",
    "#         print(f\"Fold {fold + 1}: {acc:.4f}\")\n",
    "#     print(f\"Mean accuracy: {np.mean(fold_results):.4f}  {np.std(fold_results):.4f}\")\n",
    "\n",
    "# # Start k-fold training\n",
    "# print(\"\\nStarting k-fold cross validation training...\")\n",
    "# train_with_kfold(\n",
    "#     config=config,\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, test_loader, device, num_classes):\n",
    "#     model.eval()\n",
    "#     all_preds = []\n",
    "#     all_probs = []\n",
    "#     all_targets = []\n",
    "#     class_correct = [0] * num_classes\n",
    "#     class_total = [0] * num_classes\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in tqdm(test_loader, desc='Evaluating'):\n",
    "#             inputs = inputs.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             probs = torch.softmax(outputs, dim=1)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "#             # Collect predictions and probabilities\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_probs.extend(probs.cpu().numpy())\n",
    "#             all_targets.extend(targets.numpy())\n",
    "            \n",
    "#             # Calculate class-wise accuracy\n",
    "#             correct = (preds.cpu() == targets)\n",
    "#             for i in range(len(targets)):\n",
    "#                 label = targets[i]\n",
    "#                 class_correct[label] += correct[i].item()\n",
    "#                 class_total[label] += 1\n",
    "    \n",
    "#     all_probs = np.array(all_probs)\n",
    "#     all_targets = np.array(all_targets)\n",
    "    \n",
    "#     # Print class-wise accuracy\n",
    "#     print(\"\\nClass-wise Accuracy:\")\n",
    "#     for i in range(num_classes):\n",
    "#         acc = class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
    "#         print(f'Class {i}: {acc:.4f} ({class_correct[i]}/{class_total[i]})')\n",
    "    \n",
    "#     # Plot confusion matrix\n",
    "#     cm = confusion_matrix(all_targets, all_preds)\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "#     plt.title('Confusion Matrix')\n",
    "#     plt.xlabel('Predicted')\n",
    "#     plt.ylabel('True')\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Plot ROC curves\n",
    "#     plot_roc_curves(all_targets, all_probs, num_classes)\n",
    "    \n",
    "#     # Print classification report\n",
    "#     print(\"\\nClassification Report:\")\n",
    "#     print(classification_report(all_targets, all_preds))\n",
    "    \n",
    "#     return np.mean([class_correct[i]/class_total[i] for i in range(num_classes) if class_total[i] > 0])\n",
    "\n",
    "# # Load best model\n",
    "# checkpoint = torch.load('best_model.pth')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# evaluate_model(model, val_loader, device, config['num_classes'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
