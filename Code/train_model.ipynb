{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from model_arch import HybridModel\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from dataset import KneeDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.img_size = 256  # Reduced image size for memory efficiency\n",
    "        self.batch_size = 4  # Reduced batch size to avoid memory issues\n",
    "        self.num_epochs = 20\n",
    "        self.lr = 3e-4\n",
    "        self.weight_decay = 1e-4\n",
    "        self.num_classes = 5  # KL grades: 0-4\n",
    "        self.num_workers = 4\n",
    "        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "        \n",
    "        # Transformer and model parameters\n",
    "        self.backbone = \"resnet50\"\n",
    "        self.transformer_heads = 4  # Reduced number of heads\n",
    "        self.mlp_ratio = 2  # Reduced feedforward expansion\n",
    "        self.gradient_clip_val = 1.0\n",
    "        \n",
    "        # Cross-validation\n",
    "        self.num_folds = 5\n",
    "        self.seed = 42\n",
    "        \n",
    "        # Loss weights\n",
    "        self.seg_weight = 0.5\n",
    "        self.cls_weight = 0.5\n",
    "        \n",
    "        # Paths\n",
    "        self.train_dir = \"/Users/Viku/Datasets/Medical/Knee\"  # Update this\n",
    "        self.val_dir = \"/Users/Viku/Datasets/Medical/Knee\"  # Update this\n",
    "        self.output_dir = Path(f'outputs/{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.model_dir = self.output_dir / 'models'\n",
    "        self.model_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss for segmentation with size alignment\"\"\"\n",
    "    def forward(self, pred, target, smooth=1.):\n",
    "        # Ensure pred and target have the same spatial dimensions\n",
    "        if pred.shape != target.shape:\n",
    "            pred = F.interpolate(pred, size=target.shape[2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Flatten predictions and targets\n",
    "        pred = pred.view(-1)  # Flatten segmentation prediction\n",
    "        target = target.view(-1)  # Flatten segmentation mask\n",
    "\n",
    "        # Compute Dice loss\n",
    "        intersection = (pred * target).sum()\n",
    "        dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "        return 1 - dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, seg_weight=0.5, cls_weight=0.5, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.seg_weight = seg_weight\n",
    "        self.cls_weight = cls_weight\n",
    "        self.seg_criterion = DiceLoss()\n",
    "        self.cls_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    def forward(self, seg_pred, cls_pred, seg_target, cls_target):\n",
    "        seg_loss = self.seg_criterion(seg_pred, seg_target)\n",
    "        cls_loss = self.cls_criterion(cls_pred, cls_target)\n",
    "        return self.seg_weight * seg_loss + self.cls_weight * cls_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionVisualizer:\n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "        self.attention_dir = os.path.join(output_dir, 'attention_maps')\n",
    "        os.makedirs(self.attention_dir, exist_ok=True)\n",
    "\n",
    "    def plot_attention_map(self, image, attention_map, prediction, true_grade, epoch, idx):\n",
    "        \"\"\"Visualize attention maps\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        \n",
    "        # Original image\n",
    "        axes[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "        axes[0].set_title(f'Original Image\\nPrediction: {prediction}, True Grade: {true_grade}')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # Attention map overlay\n",
    "        sns.heatmap(attention_map.mean(0).detach().cpu().numpy(), cmap='viridis', ax=axes[1])\n",
    "        axes[1].set_title('Attention Map')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(self.attention_dir, f'epoch_{epoch}_sample_{idx}.png')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history, output_dir):\n",
    "    \"\"\"Plot loss and accuracy curves\"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, history['train_loss'], label='Train Loss')\n",
    "    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'loss_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(epochs, history['val_acc'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'accuracy_curve.png'))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, fold, val_loss, config):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'val_loss': val_loss,\n",
    "    }\n",
    "    os.makedirs(config.model_dir, exist_ok=True)\n",
    "    save_path = os.path.join(config.model_dir, f'best_model_fold_{fold}_epoch_{epoch}.pth')\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Model saved at epoch {epoch} with validation loss {val_loss:.4f}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_save_button(model, optimizer, epoch, fold, val_loss, config):\n",
    "    button = widgets.Button(description=\"Save Model\")\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def on_button_click(b):\n",
    "        with output:\n",
    "            save_model(model, optimizer, epoch, fold, val_loss, config)\n",
    "\n",
    "    button.on_click(on_button_click)\n",
    "    display(button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_cross_validation(config):\n",
    "    # Check if running in notebook\n",
    "    is_notebook = True  # You can add proper detection if needed\n",
    "    num_workers = 0 if is_notebook else config.num_workers\n",
    "    \n",
    "    dataset = KneeDataset(config.train_dir, phase='train')\n",
    "    kfold = KFold(n_splits=config.num_folds, shuffle=True, random_state=config.seed)\n",
    "    \n",
    "    # Initialize attention visualizer\n",
    "    attention_visualizer = AttentionVisualizer(config.output_dir)\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_counts = dataset.class_counts\n",
    "    total_samples = sum(class_counts.values())\n",
    "    class_weights = torch.FloatTensor([\n",
    "        total_samples / (count * len(class_counts)) \n",
    "        for count in [class_counts[i] for i in range(5)]\n",
    "    ]).to(config.device)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"Starting Fold {fold + 1}/{config.num_folds}\")\n",
    "\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "        # Create weighted sampler\n",
    "        weights = dataset.get_sampling_weights()\n",
    "        train_weights = weights[train_idx]\n",
    "        sampler = WeightedRandomSampler(train_weights, len(train_weights))\n",
    "\n",
    "        # Use num_workers=0 for notebook environment\n",
    "        train_loader = DataLoader(train_subset, batch_size=config.batch_size, \n",
    "                                sampler=sampler, num_workers=num_workers)\n",
    "        val_loader = DataLoader(val_subset, batch_size=config.batch_size, \n",
    "                              shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        model = HybridModel(num_classes=config.num_classes).to(config.device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, \n",
    "                                    weight_decay=config.weight_decay)\n",
    "        criterion = CombinedLoss(seg_weight=config.seg_weight, \n",
    "                               cls_weight=config.cls_weight,\n",
    "                               class_weights=class_weights)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(config.num_epochs):\n",
    "            print(f\"Epoch [{epoch + 1}/{config.num_epochs}]\")\n",
    "\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss, correct, total = 0, 0, 0\n",
    "\n",
    "            with tqdm(train_loader, unit=\"batch\") as train_pbar:\n",
    "                for batch_idx, batch in enumerate(train_pbar):\n",
    "                    train_pbar.set_description(f\"Epoch [{epoch + 1}/{config.num_epochs}]\")\n",
    "\n",
    "                    images = batch['image'].to(config.device)\n",
    "                    masks = batch['mask'].to(config.device)\n",
    "                    grades = batch['grade'].to(config.device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    seg_out, cls_out, attention_maps = model(images, return_attention=True)  # Modified to return attention\n",
    "                    loss = criterion(seg_out, cls_out, masks, grades)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "                    _, preds = cls_out.max(1)\n",
    "                    correct += (preds == grades).sum().item()\n",
    "                    total += grades.size(0)\n",
    "\n",
    "                    # Visualize attention maps for first batch of each epoch\n",
    "                    if batch_idx == 0:\n",
    "                        for i in range(min(4, images.size(0))):  # Visualize up to 4 images\n",
    "                            attention_visualizer.plot_attention_map(\n",
    "                                images[i],\n",
    "                                attention_maps[i],\n",
    "                                preds[i].item(),\n",
    "                                grades[i].item(),\n",
    "                                epoch,\n",
    "                                f\"train_{i}\"\n",
    "                            )\n",
    "\n",
    "                    train_pbar.set_postfix({\"batch_loss\": loss.item()})\n",
    "\n",
    "            train_acc = correct / total if total > 0 else 0\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss, correct, total = 0, 0, 0\n",
    "\n",
    "            with tqdm(val_loader, unit=\"batch\") as val_pbar:\n",
    "                for batch_idx, batch in enumerate(val_pbar):\n",
    "                    val_pbar.set_description(f\"Validation [{epoch + 1}/{config.num_epochs}]\")\n",
    "\n",
    "                    images = batch['image'].to(config.device)\n",
    "                    masks = batch['mask'].to(config.device)\n",
    "                    grades = batch['grade'].to(config.device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        seg_out, cls_out, attention_maps = model(images, return_attention=True)  # Modified to return attention\n",
    "                        loss = criterion(seg_out, cls_out, masks, grades)\n",
    "\n",
    "                        val_loss += loss.item()\n",
    "                        _, preds = cls_out.max(1)\n",
    "                        correct += (preds == grades).sum().item()\n",
    "                        total += grades.size(0)\n",
    "\n",
    "                        # Visualize attention maps for first validation batch\n",
    "                        if batch_idx == 0:\n",
    "                            for i in range(min(4, images.size(0))):  # Visualize up to 4 images\n",
    "                                attention_visualizer.plot_attention_map(\n",
    "                                    images[i],\n",
    "                                    attention_maps[i],\n",
    "                                    preds[i].item(),\n",
    "                                    grades[i].item(),\n",
    "                                    epoch,\n",
    "                                    f\"val_{i}\"\n",
    "                                )\n",
    "\n",
    "                        val_pbar.set_postfix({\"batch_loss\": loss.item()})\n",
    "\n",
    "            val_acc = correct / total if total > 0 else 0\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{config.num_epochs}] - Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Train Acc: {train_acc:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # Save the best model for this fold\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                save_model(model, optimizer, epoch, fold, best_val_loss, config)\n",
    "\n",
    "    plot_metrics(history, config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5778 samples for phase: train\n",
      "Class distribution: {0: 2286, 1: 1046, 2: 1516, 3: 757, 4: 173}\n",
      "Starting Fold 1/5\n",
      "Epoch [1/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]:   0%|          | 0/1156 [00:00<?, ?batch/s]Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.6784314..0.96862745].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.26274508..0.9843137].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.99215686..-0.34117645].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.6862745..0.77254903].\n",
      "Epoch [1/20]:   7%|▋         | 84/1156 [02:04<26:27,  1.48s/batch, batch_loss=1.55] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      2\u001b[0m     config \u001b[38;5;241m=\u001b[39m Config()\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain_with_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 70\u001b[0m, in \u001b[0;36mtrain_with_cross_validation\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m seg_out, cls_out, attention_maps \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Modified to return attention\u001b[39;00m\n\u001b[1;32m     71\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(seg_out, cls_out, masks, grades)\n\u001b[1;32m     73\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/GitHub/Cloned/knee_osteoarthritis_detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/Cloned/knee_osteoarthritis_detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GitHub/Cloned/knee_osteoarthritis_detection/Code/Vignesh/model_arch.py:111\u001b[0m, in \u001b[0;36mHybridModel.forward\u001b[0;34m(self, x, return_attention)\u001b[0m\n\u001b[1;32m    108\u001b[0m seg_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseg_head(decoder_output)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Upsample to match ground truth mask shape\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m seg_out \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseg_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Classification head\u001b[39;00m\n\u001b[1;32m    114\u001b[0m cls_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_head(encoder_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/GitHub/Cloned/knee_osteoarthritis_detection/.venv/lib/python3.10/site-packages/torch/nn/functional.py:4580\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4571\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled() \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   4572\u001b[0m             \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_xpu\n\u001b[1;32m   4573\u001b[0m         ):\n\u001b[1;32m   4574\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put\u001b[39;00m\n\u001b[1;32m   4575\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   4576\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   4577\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\n\u001b[1;32m   4578\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4579\u001b[0m             )\u001b[38;5;241m.\u001b[39m_upsample_linear_vec(\u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[0;32m-> 4580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_bilinear2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4581\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\n\u001b[1;32m   4582\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4584\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = Config()\n",
    "    train_with_cross_validation(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
